# Jian Zhang

<table>
  <tr>
    <td style="width: 170px; vertical-align: top; padding-right: 20px; border: 0;">
      <div style="width: 150px; height: 150px; border-radius: 50%; overflow: hidden; position: relative;">
        <img src="jian_zhang.jpg" alt="Jian Zhang" style="display: block; height: 100%; width: auto; position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);" />
      </div>
    </td>
    <td style="vertical-align: top; border: 0;">
      <em>Graduate Student at Xiamen University, exploring Vision-Language Models (VLM) and 3D Perception.</em>
      <br><br>
      I am currently a graduate student at <a href="https://www.xmu.edu.cn/" target="_blank" rel="noopener noreferrer">Xiamen University</a> (September 2023 - Present), supervised by Professor <a href="https://huangyue05.github.io/" target="_blank" rel="noopener noreferrer">Yue Huang</a>. I obtained my Bachelor's degree in Artificial Intelligence from <a href="http://www.ncu.edu.cn/" target="_blank" rel="noopener noreferrer">Nanchang University</a> (September 2019 - June 2023).
      <br><br>
      My long-term research goal is to create AI agents capable of efficient industrial production in real-world environments, thereby significantly enhancing productivity.
    </td>
  </tr>
</table>

**Contact:**
*   [Email](mailto:zjrandomyeah@gmail.com)
*   [GitHub](https://github.com/Jian-Zhang-3DV)
*   [Google Scholar](https://scholar.google.com/citations?user=qBNtBsAAAAAJ&hl=en&oi=sra)

## Publications

### VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction (ArXiv, 2025)
*   Authors: **Jian Zhang***, Zhiwen Fan*, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, Rakesh Ranjan (*Equal Contribution)
*   Abstract: This work introduces VLM-3R, a unified Vision-Language Model (VLM) framework incorporating 3D Reconstructive instruction tuning. VLM-3R processes monocular video to derive implicit 3D tokens, aligning spatial context with language instructions for 3D spatial assistance and embodied reasoning. It also introduces the Vision-Spatial-Temporal Intelligence benchmark.
*   Links: [ğŸ“„ Paper](https://arxiv.org/abs/2505.20279) | [ğŸ’» Code](https://github.com/VITA-Group/VLM-3R) | [ğŸŒ Project Page](https://vlm-3r.github.io/)

### DynamicVerse: Physically-Aware Multimodal Modeling for Dynamic 4D Worlds (Preprint)
*   Authors: Kairun Wen*, Yuzhi Huang*, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, Chenxin Li, Wenyan Cong, **Jian Zhang**, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan (*Equal Contribution)
*   Abstract: Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with humanâ€‘like capabilities. [...] DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos.
*   Links: ğŸ“„ Paper (Coming Soon) | ğŸ’» Code (Coming Soon) | [ğŸŒ Project Page](https://dynamic-verse.github.io/)

### Large Spatial Model: End-to-end Unposed Images to Semantic 3D (NeurIPS, 2024)
*   Authors: **Jian Zhang***, Zhiwen Fan*, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang (*Equal Contribution)
*   Abstract: This work introduces the Large Spatial Model (LSM), which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass, achieving real-time semantic 3D reconstruction for the first time.
*   Links: [ğŸ“„ Paper](https://arxiv.org/abs/2410.18956) | [ğŸ’» Code](https://github.com/NVlabs/LSM) | [ğŸŒ Project Page](https://largespatialmodel.github.io/)

### InstantSplat: Sparse-view Gaussian Splatting in Seconds (ArXiv, 2024)
*   Authors: Zhiwen Fan*, Kairun Wen*, Wenyan Cong*, Kevin Wang, **Jian Zhang**, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang (*Equal Contribution)
*   Abstract: While neural 3D reconstruction has advanced substantially, its performance significantly degrades with sparse-view data, which limits its broader applicability, since SfM is often unreliable in sparse-view scenarios where feature matches are scarce. In this paper, we introduce InstantSplat, a novel approach for addressing sparse-view 3D scene reconstruction at lightning-fast speed. InstantSplat employs a self-supervised framework that optimizes 3D scene representation and camera poses by unprojecting 2D pixels into 3D space and aligning them using differentiable neural rendering.
*   Links: [ğŸ“„ Paper](https://arxiv.org/abs/2403.20309) | [ğŸ’» Code](https://github.com/NVlabs/InstantSplat) | [ğŸŒ Project Page](https://instantsplat.github.io/)
