# ğŸ‘‹ Jian Zhang

<table>
<tr>
<td width="150">
<img src="jian_zhang.jpg" alt="Jian Zhang" width="150">
</td>
<td>
<em>ğŸ“ Graduate Student at Xiamen University, exploring Vision-Language Models (VLM) and 3D Perception.</em>
<br><br>
ğŸ« I am currently a graduate student at <a href="https://www.xmu.edu.cn/">Xiamen University</a> (September 2023 - Present), supervised by Professor <a href="https://huangyue05.github.io/">Yue Huang</a>. I obtained my Bachelor's degree in Artificial Intelligence from <a href="http://www.ncu.edu.cn/">Nanchang University</a> (September 2019 - June 2023).
<br><br>
ğŸ¤– My long-term research goal is to create AI agents capable of efficient industrial production in real-world environments, thereby significantly enhancing productivity.
</td>
</tr>
</table>

**ğŸ” Looking for Opportunities:**
* ğŸŒ I am currently actively seeking research or engineering positions related to Multimodal Large Models.
* ğŸ”¬ I am particularly interested in opportunities where I can apply my research skills to real-world challenges and contribute to cutting-edge projects.
* ğŸ“© If you have any potential collaborations or suitable job openings, please feel free to contact me.

**ğŸ” æ±‚èŒæœºä¼šï¼š**
* ğŸŒ æˆ‘ç›®å‰æ­£åœ¨ç§¯æå¯»æ‰¾å¤šæ¨¡æ€å¤§æ¨¡å‹ç›¸å…³çš„ç ”ç©¶æˆ–å·¥ç¨‹å²—ä½ã€‚
* ğŸ”¬ ç‰¹åˆ«å…³æ³¨äºæå‡æ¨¡å‹åœ¨ä¸‰ç»´ç©ºé—´æ„ŸçŸ¥ã€ç†è§£ã€æ¨ç†ä¸è§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚
* ğŸ’¼ æˆ‘å¯¹èƒ½å¤Ÿå°†æˆ‘çš„ç ”ç©¶æŠ€èƒ½åº”ç”¨äºå®é™…æŒ‘æˆ˜å¹¶ä¸ºè¿™äº›é¢†åŸŸçš„å‰æ²¿é¡¹ç›®åšå‡ºè´¡çŒ®çš„æœºä¼šç‰¹åˆ«æ„Ÿå…´è¶£ã€‚
* ğŸ“© å¦‚æœæ‚¨æœ‰ä»»ä½•æ½œåœ¨çš„åˆä½œæˆ–åˆé€‚çš„å·¥ä½œæœºä¼šï¼Œè¯·éšæ—¶ä¸æˆ‘è”ç³»ã€‚

**ğŸ“¬ Contact:**
*   ğŸ“§ [Email](mailto:zjrandomyeah@gmail.com)
*   ğŸŒ [Homepage](https://jian-zhang-3dv.github.io/Jian-Zhang-3DV/)
*   ğŸ“š [Google Scholar](https://scholar.google.com/citations?user=qBNtBsAAAAAJ&hl=en&oi=sra)
*   ğŸ“„ [CV](https://jian-zhang-3dv.github.io/Jian-Zhang-3DV/cv/CV_Jian_Zhang.pdf)

## ğŸ“ Publications

### ğŸ”¥ VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction (ArXiv, 2025)
*   **Authors:** **Jian Zhang***, Zhiwen Fan*, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, Rakesh Ranjan (*Equal Contribution)
*   **Abstract:** This work introduces VLM-3R, a unified Vision-Language Model (VLM) framework incorporating 3D Reconstructive instruction tuning. VLM-3R processes monocular video to derive implicit 3D tokens, aligning spatial context with language instructions for 3D spatial assistance and embodied reasoning. It also introduces the Vision-Spatial-Temporal Intelligence benchmark.
*   **Links:** [ğŸ“„ Paper](https://arxiv.org/abs/2505.20279) | [ğŸ’» Code](https://github.com/VITA-Group/VLM-3R) | [ğŸŒ Project Page](https://vlm-3r.github.io/)

### ğŸŒŸ DynamicVerse: Physically-Aware Multimodal Modeling for Dynamic 4D Worlds (Preprint)
*   **Authors:** Kairun Wen*, Yuzhi Huang*, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, Chenxin Li, Wenyan Cong, **Jian Zhang**, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan (*Equal Contribution)
*   **Abstract:** Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with humanâ€‘like capabilities. [...] DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos.
*   **Links:** ğŸ“„ Paper (Coming Soon) | ğŸ’» Code (Coming Soon) | [ğŸŒ Project Page](https://dynamic-verse.github.io/)

### ğŸ† Large Spatial Model: End-to-end Unposed Images to Semantic 3D (NeurIPS, 2024)
*   **Authors:** **Jian Zhang***, Zhiwen Fan*, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang (*Equal Contribution)
*   **Abstract:** This work introduces the Large Spatial Model (LSM), which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass, achieving real-time semantic 3D reconstruction for the first time.
*   **Links:** [ğŸ“„ Paper](https://arxiv.org/abs/2410.18956) | [ğŸ’» Code](https://github.com/NVlabs/LSM) | [ğŸŒ Project Page](https://largespatialmodel.github.io/)

### âš¡ InstantSplat: Sparse-view Gaussian Splatting in Seconds (ArXiv, 2024)
*   **Authors:** Zhiwen Fan*, Kairun Wen*, Wenyan Cong*, Kevin Wang, **Jian Zhang**, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang (*Equal Contribution)
*   **Abstract:** While neural 3D reconstruction has advanced substantially, its performance significantly degrades with sparse-view data, which limits its broader applicability, since SfM is often unreliable in sparse-view scenarios where feature matches are scarce. In this paper, we introduce InstantSplat, a novel approach for addressing sparse-view 3D scene reconstruction at lightning-fast speed. InstantSplat employs a self-supervised framework that optimizes 3D scene representation and camera poses by unprojecting 2D pixels into 3D space and aligning them using differentiable neural rendering.
*   **Links:** [ğŸ“„ Paper](https://arxiv.org/abs/2403.20309) | [ğŸ’» Code](https://github.com/NVlabs/InstantSplat) | [ğŸŒ Project Page](https://instantsplat.github.io/)
